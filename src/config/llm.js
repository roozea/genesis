// GENESIS ‚Äî Sistema h√≠brido de LLM con auto-detecci√≥n
// Fallback chain: Local (Ollama) ‚Üí Haiku ‚Üí Sonnet ‚Üí Random

// Instrucci√≥n de idioma para Qwen (tiende a responder en chino)
const SPANISH_INSTRUCTION = 'IMPORTANTE: Responde SIEMPRE en espa√±ol. NUNCA en chino, ingl√©s u otro idioma.\n\n';

/**
 * Detecta si el texto contiene caracteres chinos
 */
function containsChinese(text) {
  if (!text) return false;
  return /[\u4e00-\u9fff]/.test(text);
}

/**
 * Fallbacks gen√©ricos en espa√±ol para cuando Ollama responde en chino
 */
const SPANISH_FALLBACKS = {
  fast: 'Explorando el lugar... üîç',
  chat: 'Hmm, d√©jame pensar en eso... ü§î',
  task: 'Procesando la informaci√≥n...',
};

// Estado global del sistema de IA
let llmState = {
  ollamaAvailable: false,
  ollamaModel: null,
  apiKeyAvailable: false,
  initialized: false,
  currentSource: 'checking', // 'local' | 'api' | 'fallback' | 'checking'
};

// Listeners para cambios de estado
const stateListeners = new Set();

// URL del proxy de Vite para Ollama (evita CORS)
const OLLAMA_PROXY_URL = '/ollama';

/**
 * Registra un listener para cambios de estado
 */
export function onStateChange(callback) {
  stateListeners.add(callback);
  // Llamar inmediatamente con el estado actual
  callback(llmState);
  return () => stateListeners.delete(callback);
}

/**
 * Notifica a todos los listeners
 */
function notifyListeners() {
  stateListeners.forEach(cb => cb(llmState));
}

/**
 * Obtiene el estado actual del LLM
 */
export function getLlmState() {
  return { ...llmState };
}

/**
 * Inicializa el sistema de LLM detectando qu√© est√° disponible
 */
export async function initLlm() {
  if (llmState.initialized) {
    return llmState;
  }

  console.log('[llm] Iniciando auto-detecci√≥n...');

  // 1. Detectar Ollama
  llmState.ollamaAvailable = await checkOllama();
  if (llmState.ollamaAvailable) {
    llmState.ollamaModel = await getOllamaModel();
    console.log(`[llm] ‚úì Ollama disponible con modelo: ${llmState.ollamaModel}`);
  } else {
    console.log('[llm] ‚úó Ollama no disponible');
  }

  // 2. Detectar API Key
  const apiKey = import.meta.env.VITE_ANTHROPIC_API_KEY;
  llmState.apiKeyAvailable = !!(apiKey && apiKey.length > 10 && apiKey !== 'tu-api-key-aqui');
  if (llmState.apiKeyAvailable) {
    console.log('[llm] ‚úì API Key configurada');
  } else {
    console.log('[llm] ‚úó API Key no configurada');
  }

  // 3. Determinar fuente principal
  if (llmState.ollamaAvailable && llmState.ollamaModel) {
    llmState.currentSource = 'local';
  } else if (llmState.apiKeyAvailable) {
    llmState.currentSource = 'api';
  } else {
    llmState.currentSource = 'fallback';
  }

  llmState.initialized = true;
  notifyListeners();

  console.log(`[llm] Modo activo: ${llmState.currentSource.toUpperCase()}`);
  return llmState;
}

/**
 * Verifica si Ollama est√° disponible
 */
async function checkOllama() {
  try {
    const response = await fetch(`${OLLAMA_PROXY_URL}/api/tags`, {
      method: 'GET',
      signal: AbortSignal.timeout(3000),
    });
    return response.ok;
  } catch (error) {
    return false;
  }
}

/**
 * Obtiene el modelo de Ollama disponible (preferencia: qwen2.5:7b)
 */
async function getOllamaModel() {
  try {
    const response = await fetch(`${OLLAMA_PROXY_URL}/api/tags`);
    if (!response.ok) return null;

    const data = await response.json();
    const models = data.models || [];

    // Buscar qwen2.5:7b primero
    const qwen = models.find(m => m.name.includes('qwen2.5:7b'));
    if (qwen) return qwen.name;

    // Luego cualquier qwen
    const anyQwen = models.find(m => m.name.includes('qwen'));
    if (anyQwen) return anyQwen.name;

    // Cualquier modelo disponible
    if (models.length > 0) return models[0].name;

    return null;
  } catch {
    return null;
  }
}

/**
 * Llama a Ollama (local)
 * @param {string} systemPrompt - System prompt
 * @param {string} userMessage - Mensaje del usuario
 * @param {string} tier - 'fast' para movimiento, 'chat' para conversaci√≥n, 'task' para trabajo
 * @param {number} maxTokens - L√≠mite de tokens
 * @param {boolean} forceSpanish - Si es true, refuerza m√°s la instrucci√≥n de espa√±ol (retry)
 */
async function callOllama(systemPrompt, userMessage, tier = 'fast', maxTokens = 100, forceSpanish = false) {
  // Agregar instrucci√≥n de espa√±ol al inicio (Qwen es biling√ºe y a veces responde en chino)
  const spanishPrefix = forceSpanish
    ? 'EN ESPA√ëOL SOLAMENTE (NO CHINO): '
    : SPANISH_INSTRUCTION;

  // Formato de prompt limpio para Ollama
  const prompt = (tier === 'chat' || tier === 'task')
    ? `${spanishPrefix}${systemPrompt}\n\nRodrigo dice: "${userMessage}"\n\nArq responde (en espa√±ol):`
    : `${spanishPrefix}${systemPrompt}\n\n${userMessage}`;

  const response = await fetch(`${OLLAMA_PROXY_URL}/api/generate`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model: llmState.ollamaModel || 'qwen2.5:7b',
      prompt,
      stream: false,
      options: {
        temperature: 0.7,
        num_predict: maxTokens,
      },
    }),
    signal: AbortSignal.timeout(tier === 'task' ? 30000 : 15000), // 30s para task, 15s normal
  });

  if (!response.ok) {
    throw new Error(`Ollama error: ${response.status}`);
  }

  const data = await response.json();
  return data.response || '';
}

/**
 * Llama a la API de Anthropic
 * @param {number} maxTokens - L√≠mite de tokens (default seg√∫n modelo)
 */
async function callAnthropic(systemPrompt, userMessage, model = 'haiku', maxTokens = null) {
  const apiKey = import.meta.env.VITE_ANTHROPIC_API_KEY;
  if (!apiKey || apiKey === 'tu-api-key-aqui') {
    throw new Error('API Key no configurada');
  }

  const modelId = model === 'haiku'
    ? 'claude-3-haiku-20240307'
    : 'claude-3-5-sonnet-20241022';

  // Tokens por defecto seg√∫n modelo, o usar el valor expl√≠cito
  const tokens = maxTokens || (model === 'haiku' ? 200 : 600);

  const response = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': apiKey,
      'anthropic-version': '2023-06-01',
      'anthropic-dangerous-direct-browser-access': 'true',
    },
    body: JSON.stringify({
      model: modelId,
      max_tokens: tokens,
      system: systemPrompt,
      messages: [{ role: 'user', content: userMessage }],
    }),
  });

  if (!response.ok) {
    const error = await response.json().catch(() => ({}));
    throw new Error(`API Error: ${response.status} - ${error.error?.message || 'Unknown'}`);
  }

  const data = await response.json();
  return data.content[0]?.text || '';
}

/**
 * Funci√≥n principal de pensamiento con fallback chain
 * @param {string} systemPrompt - System prompt
 * @param {string} userMessage - Mensaje del usuario
 * @param {'fast' | 'chat' | 'task'} tier - Tier de modelo (fast=movimiento, chat=conversaci√≥n, task=trabajo largo)
 * @returns {Promise<{response: string, source: string}>}
 */
export async function think(systemPrompt, userMessage, tier = 'fast') {
  // Asegurar inicializaci√≥n
  if (!llmState.initialized) {
    await initLlm();
  }

  // Tokens seg√∫n tier
  const maxTokens = tier === 'task' ? 800 : tier === 'chat' ? 400 : 150;
  const ollamaTokens = tier === 'task' ? 500 : tier === 'chat' ? 250 : 100;

  // Fallback chain: LOCAL SIEMPRE PRIMERO para todos los tiers
  // fast = movimiento (respuestas cortas)
  // chat = conversaci√≥n (respuestas m√°s elaboradas)
  // task = trabajo/deliverables (respuestas largas)
  const chain = ['local', 'haiku', 'sonnet', 'fallback'];

  console.log(`[LLM] think() tier=${tier}, tokens=${tier === 'fast' ? ollamaTokens : maxTokens}`);

  for (const source of chain) {
    try {
      let response;

      switch (source) {
        case 'local':
          if (!llmState.ollamaAvailable || !llmState.ollamaModel) {
            console.log('[LLM] local: saltando (no disponible)');
            continue;
          }
          console.log(`[LLM] local: intentando con ${llmState.ollamaModel}...`);
          response = await callOllama(systemPrompt, userMessage, tier, ollamaTokens);
          console.log('[LLM] local: respuesta:', response?.slice(0, 80) || '(vac√≠a)');

          // Validar si respondi√≥ en chino (Qwen es biling√ºe)
          if (response && containsChinese(response)) {
            console.warn('[LLM] ‚ö†Ô∏è Respuesta en chino detectada, reintentando con espa√±ol forzado...');
            try {
              response = await callOllama(systemPrompt, userMessage, tier, ollamaTokens, true);
              console.log('[LLM] local retry:', response?.slice(0, 80) || '(vac√≠a)');

              // Si sigue en chino, usar fallback espa√±ol
              if (containsChinese(response)) {
                console.warn('[LLM] ‚ö†Ô∏è Sigue en chino, usando fallback espa√±ol');
                response = SPANISH_FALLBACKS[tier] || SPANISH_FALLBACKS.fast;
              }
            } catch (retryError) {
              console.error('[LLM] Retry fall√≥:', retryError.message);
              response = SPANISH_FALLBACKS[tier] || SPANISH_FALLBACKS.fast;
            }
          }

          if (response && response.trim()) {
            return { response, source: 'local' };
          }
          console.log('[LLM] local: respuesta vac√≠a, continuando...');
          break;

        case 'haiku':
          if (!llmState.apiKeyAvailable) {
            console.log('[LLM] haiku: saltando (no API key)');
            continue;
          }
          console.log('[LLM] haiku: intentando...');
          response = await callAnthropic(systemPrompt, userMessage, 'haiku', maxTokens);
          console.log('[LLM] haiku: respuesta:', response?.slice(0, 80) || '(vac√≠a)');
          if (response && response.trim()) {
            return { response, source: 'haiku' };
          }
          break;

        case 'sonnet':
          if (!llmState.apiKeyAvailable) {
            console.log('[LLM] sonnet: saltando (no API key)');
            continue;
          }
          console.log('[LLM] sonnet: intentando...');
          response = await callAnthropic(systemPrompt, userMessage, 'sonnet', maxTokens);
          console.log('[LLM] sonnet: respuesta:', response?.slice(0, 80) || '(vac√≠a)');
          if (response && response.trim()) {
            return { response, source: 'sonnet' };
          }
          break;

        case 'fallback':
          console.log('[LLM] ‚ùå Llegando a fallback (nada funcion√≥)');
          return { response: null, source: 'fallback' };
      }
    } catch (error) {
      console.error(`[LLM] ‚ùå ${source} fall√≥:`, error.message);
      // Continuar con el siguiente en la cadena
    }
  }

  console.log('[LLM] ‚ùå Chain completo sin √©xito');
  return { response: null, source: 'fallback' };
}

/**
 * Versi√≥n simple que solo retorna el texto (compatibilidad)
 */
export async function thinkSimple(systemPrompt, userMessage, tier = 'fast') {
  const result = await think(systemPrompt, userMessage, tier);
  return result.response;
}

// Auto-inicializar al cargar el m√≥dulo
initLlm();
